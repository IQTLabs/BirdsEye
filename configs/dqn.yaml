[Methods]
method : dqn
action : simpleactions
sensor : drone
state : rfstate 
target_speed: 1.0
target_start: 100
resample_proportion: 0.1
model: small
#target_speed_range: [0,1,2,3,4]
#reward: entropy_collision_reward

[Defaults]
number_timesteps : 100000
dueling : True
double_q : True
param_noise : True
exploration_fraction : 0.2
exploration_final_eps : 0.1
batch_size : 64
train_freq : 4
# was 4 
learning_starts : 100
target_network_update_freq : 600
# was 100
buffer_size : 10000
prioritized_replay : True
prioritized_replay_alpha : 0.6
prioritized_replay_beta0 : 0.4
min_value : -10
max_value : 10
max_episode_length : 150
atom_num : 1
ob_scale : 1
gamma : 0.99
grad_norm : 10.0
save_interval : 100000
#10000
save_path : checkpoints
log_path : rl_log
use_gpu : True
plotting : False
